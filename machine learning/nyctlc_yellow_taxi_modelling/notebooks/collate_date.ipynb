{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/08/22 16:23:25 WARN Utils: Your hostname, LAPTOP-VAB0S7AL resolves to a loopback address: 127.0.1.1; using 172.27.239.27 instead (on interface eth0)\n",
      "22/08/22 16:23:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/noahs/miniconda3/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/08/22 16:23:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/08/22 16:23:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034_Yellow_Taxi_Preprocessing_Feature_Engineering\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " VendorID              | 1                   \n",
      " tpep_pickup_datetime  | 2019-03-01 00:24:41 \n",
      " tpep_dropoff_datetime | 2019-03-01 00:25:31 \n",
      " passenger_count       | 1.0                 \n",
      " trip_distance         | 0.0                 \n",
      " RatecodeID            | 1.0                 \n",
      " store_and_fwd_flag    | N                   \n",
      " PULocationID          | 145                 \n",
      " DOLocationID          | 145                 \n",
      " payment_type          | 2                   \n",
      " fare_amount           | 2.5                 \n",
      " extra                 | 0.5                 \n",
      " mta_tax               | 0.5                 \n",
      " tip_amount            | 0.0                 \n",
      " tolls_amount          | 0.0                 \n",
      " improvement_surcharge | 0.3                 \n",
      " total_amount          | 3.8                 \n",
      " congestion_surcharge  | 0.0                 \n",
      " airport_fee           | null                \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read in 2019 Taxi data -> this will be used for training\n",
    "sdf_full = spark.read.parquet('../data/raw/tlc_data/tlc_data_yellow/2019*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sdf_full.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sdf_full.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from scripts._global_vars import *\n",
    "from scripts.collate import drop_cast_and_create_taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sdf = drop_cast_and_create_taxi(sdf_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- PU_datetime: date (nullable = true)\n",
      " |-- DO_datetime: date (nullable = true)\n",
      " |-- PU_hourofday: integer (nullable = true)\n",
      " |-- DO_hourofday: integer (nullable = true)\n",
      " |-- PU_dayofweek: integer (nullable = true)\n",
      " |-- DO_dayofweek: integer (nullable = true)\n",
      " |-- PU_dayofmonth: integer (nullable = true)\n",
      " |-- DO_dayofmonth: integer (nullable = true)\n",
      " |-- PU_month: integer (nullable = true)\n",
      " |-- DO_month: integer (nullable = true)\n",
      " |-- trip_time_minutes: double (nullable = true)\n",
      " |-- trip_speed_mph: double (nullable = true)\n",
      " |-- fare_per_minute: double (nullable = true)\n",
      " |-- hour_of_day_of_year: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/08/22 16:24:10 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>VendorID</th><th>tpep_pickup_datetime</th><th>tpep_dropoff_datetime</th><th>passenger_count</th><th>trip_distance</th><th>RatecodeID</th><th>PULocationID</th><th>DOLocationID</th><th>payment_type</th><th>fare_amount</th><th>tip_amount</th><th>congestion_surcharge</th><th>PU_datetime</th><th>DO_datetime</th><th>PU_hourofday</th><th>DO_hourofday</th><th>PU_dayofweek</th><th>DO_dayofweek</th><th>PU_dayofmonth</th><th>DO_dayofmonth</th><th>PU_month</th><th>DO_month</th><th>trip_time_minutes</th><th>trip_speed_mph</th><th>fare_per_minute</th><th>hour_of_day_of_year</th></tr>\n",
       "<tr><td>1</td><td>2019-03-01 00:24:41</td><td>2019-03-01 00:25:31</td><td>1.0</td><td>0.0</td><td>1</td><td>145</td><td>145</td><td>2</td><td>2.5</td><td>0.0</td><td>0.0</td><td>2019-03-01</td><td>2019-03-01</td><td>0</td><td>0</td><td>6</td><td>1</td><td>1</td><td>1</td><td>3</td><td>3</td><td>0.8333333333333334</td><td>0.0</td><td>3.0</td><td>2019-03-01 00:00:00</td></tr>\n",
       "<tr><td>1</td><td>2019-03-01 00:25:27</td><td>2019-03-01 00:36:37</td><td>2.0</td><td>3.7</td><td>1</td><td>95</td><td>130</td><td>1</td><td>13.0</td><td>0.7</td><td>0.0</td><td>2019-03-01</td><td>2019-03-01</td><td>0</td><td>0</td><td>6</td><td>1</td><td>1</td><td>1</td><td>3</td><td>3</td><td>11.166666666666666</td><td>19.880597014925378</td><td>1.164179104477612</td><td>2019-03-01 00:00:00</td></tr>\n",
       "<tr><td>1</td><td>2019-03-01 00:05:21</td><td>2019-03-01 00:38:23</td><td>1.0</td><td>14.1</td><td>1</td><td>249</td><td>28</td><td>1</td><td>41.0</td><td>10.1</td><td>2.5</td><td>2019-03-01</td><td>2019-03-01</td><td>0</td><td>0</td><td>6</td><td>1</td><td>1</td><td>1</td><td>3</td><td>3</td><td>33.03333333333333</td><td>25.610494450050453</td><td>1.24117053481332</td><td>2019-03-01 00:00:00</td></tr>\n",
       "<tr><td>1</td><td>2019-03-01 00:48:55</td><td>2019-03-01 01:06:03</td><td>1.0</td><td>9.6</td><td>1</td><td>138</td><td>98</td><td>2</td><td>27.0</td><td>0.0</td><td>0.0</td><td>2019-03-01</td><td>2019-03-01</td><td>0</td><td>1</td><td>6</td><td>1</td><td>1</td><td>1</td><td>3</td><td>3</td><td>17.133333333333333</td><td>33.61867704280156</td><td>1.575875486381323</td><td>2019-03-01 00:00:00</td></tr>\n",
       "<tr><td>1</td><td>2019-03-01 00:11:42</td><td>2019-03-01 00:16:40</td><td>1.0</td><td>0.8</td><td>1</td><td>48</td><td>48</td><td>1</td><td>5.5</td><td>3.0</td><td>2.5</td><td>2019-03-01</td><td>2019-03-01</td><td>0</td><td>0</td><td>6</td><td>1</td><td>1</td><td>1</td><td>3</td><td>3</td><td>4.966666666666667</td><td>9.664429530201343</td><td>1.1073825503355705</td><td>2019-03-01 00:00:00</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------+------------+------------+-----------+----------+--------------------+-----------+-----------+------------+------------+------------+------------+-------------+-------------+--------+--------+------------------+------------------+------------------+-------------------+\n",
       "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|PULocationID|DOLocationID|payment_type|fare_amount|tip_amount|congestion_surcharge|PU_datetime|DO_datetime|PU_hourofday|DO_hourofday|PU_dayofweek|DO_dayofweek|PU_dayofmonth|DO_dayofmonth|PU_month|DO_month| trip_time_minutes|    trip_speed_mph|   fare_per_minute|hour_of_day_of_year|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------+------------+------------+-----------+----------+--------------------+-----------+-----------+------------+------------+------------+------------+-------------+-------------+--------+--------+------------------+------------------+------------------+-------------------+\n",
       "|       1| 2019-03-01 00:24:41|  2019-03-01 00:25:31|            1.0|          0.0|         1|         145|         145|           2|        2.5|       0.0|                 0.0| 2019-03-01| 2019-03-01|           0|           0|           6|           1|            1|            1|       3|       3|0.8333333333333334|               0.0|               3.0|2019-03-01 00:00:00|\n",
       "|       1| 2019-03-01 00:25:27|  2019-03-01 00:36:37|            2.0|          3.7|         1|          95|         130|           1|       13.0|       0.7|                 0.0| 2019-03-01| 2019-03-01|           0|           0|           6|           1|            1|            1|       3|       3|11.166666666666666|19.880597014925378| 1.164179104477612|2019-03-01 00:00:00|\n",
       "|       1| 2019-03-01 00:05:21|  2019-03-01 00:38:23|            1.0|         14.1|         1|         249|          28|           1|       41.0|      10.1|                 2.5| 2019-03-01| 2019-03-01|           0|           0|           6|           1|            1|            1|       3|       3| 33.03333333333333|25.610494450050453|  1.24117053481332|2019-03-01 00:00:00|\n",
       "|       1| 2019-03-01 00:48:55|  2019-03-01 01:06:03|            1.0|          9.6|         1|         138|          98|           2|       27.0|       0.0|                 0.0| 2019-03-01| 2019-03-01|           0|           1|           6|           1|            1|            1|       3|       3|17.133333333333333| 33.61867704280156| 1.575875486381323|2019-03-01 00:00:00|\n",
       "|       1| 2019-03-01 00:11:42|  2019-03-01 00:16:40|            1.0|          0.8|         1|          48|          48|           1|        5.5|       3.0|                 2.5| 2019-03-01| 2019-03-01|           0|           0|           6|           1|            1|            1|       3|       3| 4.966666666666667| 9.664429530201343|1.1073825503355705|2019-03-01 00:00:00|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------+------------+------------+-----------+----------+--------------------+-----------+-----------+------------+------------+------------+------------+-------------+-------------+--------+--------+------------------+------------------+------------------+-------------------+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify\n",
    "#sdf.printSchema()\n",
    "#sdf.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:====================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+--------------------+\n",
      "|summary|       fare_amount|     trip_distance|   passenger_count|        tip_amount|congestion_surcharge|\n",
      "+-------+------------------+------------------+------------------+------------------+--------------------+\n",
      "|  count|          84598444|          84598444|          84154061|          84598444|            79297843|\n",
      "|   mean|13.412639732835764|3.0183506184817515|1.5626654190817957| 2.190078737505638|  2.1949917301029234|\n",
      "| stddev|174.17668755385404| 8.093902044464816|1.2079081585219809|15.638996154306168|  0.8296498809008713|\n",
      "|    min|           -1856.0|         -37264.53|               0.0|            -221.0|                -2.5|\n",
      "|    max|          943274.8|          45977.22|               9.0|         141492.02|                 4.5|\n",
      "+-------+------------------+------------------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Pre cleaning descriptive statistics\n",
    "sdf.describe(*non_categorical_features).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Weather data aggregation + joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# read in weather data\n",
    "weather_date_path = \"../data/raw/nyc_weather_date/NYC.csv\"\n",
    "weather_sdf = spark.read.csv(weather_date_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station: string (nullable = true)\n",
      " |-- valid: string (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- tmpf: string (nullable = true)\n",
      " |-- dwpf: string (nullable = true)\n",
      " |-- relh: string (nullable = true)\n",
      " |-- drct: string (nullable = true)\n",
      " |-- sknt: string (nullable = true)\n",
      " |-- p01i: string (nullable = true)\n",
      " |-- alti: string (nullable = true)\n",
      " |-- mslp: string (nullable = true)\n",
      " |-- vsby: string (nullable = true)\n",
      " |-- gust: string (nullable = true)\n",
      " |-- skyc1: string (nullable = true)\n",
      " |-- skyc2: string (nullable = true)\n",
      " |-- skyc3: string (nullable = true)\n",
      " |-- skyc4: string (nullable = true)\n",
      " |-- skyl1: string (nullable = true)\n",
      " |-- skyl2: string (nullable = true)\n",
      " |-- skyl3: string (nullable = true)\n",
      " |-- skyl4: string (nullable = true)\n",
      " |-- wxcodes: string (nullable = true)\n",
      " |-- ice_accretion_1hr: string (nullable = true)\n",
      " |-- ice_accretion_3hr: string (nullable = true)\n",
      " |-- ice_accretion_6hr: string (nullable = true)\n",
      " |-- peak_wind_gust: string (nullable = true)\n",
      " |-- peak_wind_drct: string (nullable = true)\n",
      " |-- peak_wind_time: string (nullable = true)\n",
      " |-- feel: string (nullable = true)\n",
      " |-- metar: string (nullable = true)\n",
      " |-- snowdepth: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scripts.collate import weather_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "weather_sdf = weather_process(weather_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hour_of_day_of_year: timestamp (nullable = true)\n",
      " |-- tmpf: double (nullable = true)\n",
      " |-- dwpf: double (nullable = true)\n",
      " |-- relh: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "weather_sdf.count()\n",
    "weather_sdf.printSchema()\n",
    "# few missing values -> we dont have to worry -> time to join datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "export_relative_dir = '../data/curated/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Export processed weather data\n",
    "weather_sdf.write.mode('overwrite').csv(export_relative_dir + \"weather_data_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Collate weather and NYCTLC dataset\n",
    "combined_sdf = sdf.join(weather_sdf, on=['hour_of_day_of_year'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export completed\n"
     ]
    }
   ],
   "source": [
    "combined_sdf.write.mode('overwrite').parquet(export_relative_dir + \"yt2019_feature_eng.parquet\")\n",
    "print(\"Export completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CLEAR CACHE\")\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}